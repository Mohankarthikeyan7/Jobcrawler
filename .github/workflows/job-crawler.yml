name: UK Company Job Crawler

on:
  schedule:
    # Run every 10 minutes
    - cron: '*/10 * * * *'
  workflow_dispatch:  # Allow manual trigger
    inputs:
      max_companies:
        description: 'Maximum companies to process'
        required: false
        default: '10'
        type: string

env:
  TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
  TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
  EXCEL_FILE: companies.xlsx

jobs:
  crawl-jobs:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache processed companies
      uses: actions/cache@v4
      with:
        path: |
          processed_companies.json
          failed_companies.json
        key: companies-cache-v1
        restore-keys: |
          companies-cache-v1
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Check if Excel file exists
      run: |
        if [ ! -f "$EXCEL_FILE" ]; then
          echo "Error: Excel file $EXCEL_FILE not found!"
          echo "Please add your companies.xlsx file to the repository root."
          exit 1
        fi
        
    - name: Run job crawler
      run: |
        python job_crawler.py
        
    - name: Show progress summary
      run: |
        echo "=== CRAWLER PROGRESS SUMMARY ==="
        if [ -f "processed_companies.json" ]; then
          echo "Successfully processed companies: $(cat processed_companies.json | jq length 2>/dev/null || echo 0)"
        else
          echo "Successfully processed companies: 0"
        fi
        if [ -f "failed_companies.json" ]; then
          echo "Failed companies: $(cat failed_companies.json | jq length 2>/dev/null || echo 0)"
        else
          echo "Failed companies: 0"
        fi
        echo "================================"

    - name: Upload progress files as artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: crawler-progress-${{ github.run_number }}
        path: |
          processed_companies.json
          failed_companies.json
          *.log
        retention-days: 7
